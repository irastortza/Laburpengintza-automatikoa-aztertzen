{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laburpengintza automatikoa aztertzen\n",
        "\n",
        "Proiektu honetan laburpengintza automatikoaren oinarriak aztertu dira. Laburpengintza automatikoa historikoki bi arlotan banatu da: erauzketa bidezko laburpengintza eta abstrakzio bidezko laburpengintza. Proiektu honetan bi arloak aztertu eta alderatu dira, horretarako arlo bakoitzeko sistema bat diseinatu, entrenatu eta probatuz. \n",
        "\n",
        "## Proiektauren ingurunea sortzea\n",
        "\n",
        "Zati honetan proiektuan zehar lan egin den ingurunea sortzen da. Inprotazioak egin, datu-baseak saretik eskuratu eta zenbait funtzio definitzen dira, jarraian xeheago azalduta. "
      ],
      "metadata": {
        "id": "oX7zG_WaGvAj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG5xHOZXGou2"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install -U transformers\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install rouge\n",
        "!pip install datasets\n",
        "!pip install simplet5 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline \n",
        "from rouge import Rouge \n",
        "from datasets import list_datasets, load_dataset\n",
        "from pprint import pprint\n",
        "import spacy, glob, os\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "from simplet5 import SimpleT5\n",
        "\n",
        "from contextlib import ExitStack\n",
        "from itertools import zip_longest\n",
        "import re, string\n",
        "\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "yLGtv9d7HaoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1TyU3cqv53xeY0OkuqZ-oyeH_mEKBGSpN' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1TyU3cqv53xeY0OkuqZ-oyeH_mEKBGSpN\" -O glove.txt && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=143QJ0oWA7ScFY8gLe6JOLInEcO1oheUu' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=143QJ0oWA7ScFY8gLe6JOLInEcO1oheUu\" -O 15000.zip && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SCCyXaNtiSoguXSqkHd2EqQdUTNZC1Ha' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1SCCyXaNtiSoguXSqkHd2EqQdUTNZC1Ha\" -O laburtzailea.pk && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1VG3uBfmgBPQaa3BJGPKfFW4Z6SBDbrS3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1VG3uBfmgBPQaa3BJGPKfFW4Z6SBDbrS3\" -O testa.zip && rm -rf /tmp/cookies.txt\n",
        "!mkdir \"/content/esaldiak\"\n",
        "!unzip -q 15000.zip #2:28 minutu (aurreprozesatutako datu-basea)\n",
        "!unzip -q testa.zip #25 segundo (testerako datu-basea)"
      ],
      "metadata": {
        "id": "GMNrvwq1AEka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Azpian Spacy-k eskaintzen dituen tresnak definitu dira. Proiektu honetan Spacy-ren tokenizatzailea eta Spacy-ren esaldi-bereizlea erabiliko dira hitzak banatzeko eta esaldiak banatzeko, hurrenez hurren. "
      ],
      "metadata": {
        "id": "NrMOUpjEHmWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "sentencizer = nlp.add_pipe(\"sentencizer\")"
      ],
      "metadata": {
        "id": "eJFGK0EDHekE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gomendagarria da azpiko kodea azkarrago eta eraginkorrago exekutatzeko GPUa erabiliz egitea, kode ugari paralelizatuta baitago."
      ],
      "metadata": {
        "id": "QZHhrwBuIQD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "s4ZwHA_8Hf1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datu-base bezala \"multi_news\" erabiliko dugu, zeinak [newser.com](https://www.newser.com/) ingelesezko egunkariko artikuluak biltzen dituen, haien laburpenekin batera. Laburpen hauek eskuz gizakiek idatziak izan dira eta beraz, zuzenak izatearen bermea dute kasu ia guztietan. \n",
        "\n",
        "Datu-basea jada partizio-banaketa eginda dator. Entrenamendurako artikuluen %80a gordetzen du, garapenerako %10a eta gainerako %10a testerako. Datu-baseko adibide bakoitzak bi zati ditu: artikulua bera (\"document\") eta haren laburpena (\"summary\"). "
      ],
      "metadata": {
        "id": "XS9NHutYIXp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('multi_news')\n",
        "entrenamendua = dataset[\"train\"]\n",
        "garapena = dataset[\"validation\"]\n",
        "testa = dataset[\"test\"]\n",
        "\n",
        "print(\"\\n\\nDATUBASEAREN EZAUGARRIAK\")\n",
        "print(\"\\nAdibidea:\\n\\n\")\n",
        "print(\"***Artikulua: \",entrenamendua[0][\"document\"])\n",
        "print(\"\\n\\n***Laburpena: \",entrenamendua[0][\"summary\"])\n",
        "print(\"-----------------------\")\n",
        "print(\"Entrenamendua: \", len(entrenamendua), \"artikulu (%80).\")\n",
        "print(\"Garapena: \",len(garapena),\"artikulu (%10).\")\n",
        "print(\"Testa: \",len(testa),\"artikulu (%10). \")"
      ],
      "metadata": {
        "id": "Oi4ZP7-mHhfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lan egingo dugun datu-basearen gaineko ezaugarri gehiago jakitea ez dator soberan. Azpiko kodeak ezaugarri gehigarriak erakusten ditu partizio bakoitzeko. \n",
        "\n",
        "Ikusten den bezala, 10 adibide baliogabe daude guztira diren 56.216 artikuluen artean. Artikuluen batez besteko luzera, espero genuen bezala, antzekoa da partizio guztien artean eta hori 10.756,32 eta 11.067,97 karaktereen artekoa da. Laburpenen batez besteko luzera ere orekatuta dago partizioen artean eta artikuluen luzeraren hamarren bat da gutxi gorabehera (1.290,66 eta 1.300,30 karaktereen artekoa). Bestalde, ikus daiteke artikulu batzuek (akatsdunak) zeroko luzera dutela eta beste batzuek ohi baino 200 aldiz luzera handiagoa, gutxi gorabehera. Laburpenen artean ez dago luzera-desberdintasun hain handia. Xehetasun hauek kontuan hartu beharko dira aurreprozesaketa egitean. "
      ],
      "metadata": {
        "id": "mFeYRt5RJlhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aztertuDatubasea (datuak):\n",
        "  artikuluKop=len(datuak) #Hasieraketak\n",
        "  artikuluLuzeena=0\n",
        "  laburpenLuzeena=0\n",
        "  artikuluMotzena=len(datuak[0][\"document\"])\n",
        "  laburpenMotzena=len(datuak[0][\"summary\"])\n",
        "  artikuluLuzerak=0\n",
        "  laburpenLuzerak=0\n",
        "  artLabErlazioAlt=len(datuak[0][\"summary\"])/len(datuak[0][\"document\"])\n",
        "  artLabErlazioBax=len(datuak[0][\"summary\"])/len(datuak[0][\"document\"])\n",
        "  adibideBaliogabeak=0\n",
        "  for i in datuak: #Dokumentu bakoitzeko egin\n",
        "    try: #Kontuz dokumentu akatsdunekin, zero luzerakoak izan daitezke.\n",
        "      artikuluLuzerak=artikuluLuzerak+len(i[\"document\"])\n",
        "      laburpenLuzerak=laburpenLuzerak+len(i[\"summary\"])\n",
        "      if len(i[\"document\"]) < artikuluMotzena: #Artikulu luzera minimoa\n",
        "        artikuluMotzena=len(i[\"document\"])\n",
        "      elif len(i[\"document\"]) > artikuluLuzeena: #Artikulu luzera maximoa\n",
        "        artikuluLuzeena=len(i[\"document\"])\n",
        "      if len(i[\"summary\"]) < laburpenMotzena: #Laburpen luzera minimoa\n",
        "        laburpenMotzena=len(i[\"summary\"])\n",
        "      elif len(i[\"summary\"]) > laburpenLuzeena: #Laburpen luzera maximoa\n",
        "        laburpenLuzeena=len(i[\"summary\"])\n",
        "      if (len(i[\"summary\"]) / len(i[\"document\"]) < artLabErlazioBax): #Artikulu luzera / laburpen luzera minimoa \n",
        "        artLabErlazioBax=len(i[\"summary\"]) / len(i[\"document\"])\n",
        "      elif (len(i[\"summary\"]) / len(i[\"document\"]) > artLabErlazioAlt): #Artikulu luzera / laburpen luzera maximoa\n",
        "        artLabErlazioAlt=len(i[\"summary\"]) / len(i[\"document\"])\n",
        "    except ZeroDivisionError:\n",
        "      adibideBaliogabeak+=1\n",
        "  print(\"Adibide baliogabeak (dokumentu hutsak, etab.): \",adibideBaliogabeak)\n",
        "  print(\"Artikuluen guztizko luzera: \",artikuluLuzerak, \"batez beste: \", artikuluLuzerak / artikuluKop)\n",
        "  print(\"Laburpenen guztizko luzera: \",laburpenLuzerak, \"batez beste: \", laburpenLuzerak / artikuluKop)\n",
        "  print(\"Artikulu motzenaren luzera: \", artikuluMotzena, \" eta luzeenaren luzera: \",artikuluLuzeena)\n",
        "  print(\"Laburpen motzenaren luzera: \", laburpenMotzena,\" eta luzeenaren luzera: \",laburpenLuzeena)\n",
        "  print(\"Laburpenenaren eta artikuluaren tamainaren arteko erlazio baxuena: \",artLabErlazioBax, \"eta altuena: \", artLabErlazioAlt)\n",
        "\n",
        "print(\"ENTRENAMENDURAKO DATUBASEAREN EZAUGARRI XEHEAK: \")\n",
        "aztertuDatubasea(entrenamendua)\n",
        "print(\"\\nGARAPENERAKO DATUBASEAREN EZAUGARRI XEHEAK: \")\n",
        "aztertuDatubasea(garapena)\n",
        "print(\"\\nTESTERAKO DATUBASEAREN EZAUGARRI XEHEAK: \")\n",
        "aztertuDatubasea(testa)"
      ],
      "metadata": {
        "id": "ifn_zdIcJay4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Erauzketa bidezko laburpengintza\n",
        "\n",
        "Atal honetan erauzketa bidez lan egingo duen laburtzaile automatiko bat sortzen saiatuko gara. Laburtzaile automatiko mota honek testu bateko esaldi garrantzitsuenak atzeman eta horiek itzultzen ditu soilik, elkartuta. Laburpengintza automatikoaren historian interes gehien jarri zaion metodoa izan da, bere sinpletasun eta zenbaitetan eraginkortasunarengatik. \n",
        "\n",
        "Erauzketa baliatzen duten laburtzaile automatiko gehienak sortzeko metodo ez-gainbegiratuak erabiltzen dira, baina ikasgai honetan metodo gainbegiratuak landu direnez, haiek erabiltzen saiatuko gara laburtzaile automatikoa sortzerako orduan. Sistema sortzeko erabiliko den arkitektura neurona-sare bat izango da, LSTM arkitekturan inspiratua eta SFNet izenez ezaguna, zeinbait aldaketekin. \n",
        "\n",
        "### Embedding-en hiztegiaren sorkuntza\n",
        "\n",
        "Azpiko kodeak Glove-ko hiztegia erabilita hitz bakoitzari identifikatzaile bat esleitzen dio, ondoren embedding bihurtzeko. Aurreprozesaketaren parte da. "
      ],
      "metadata": {
        "id": "rI4lz0O5LAwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab,inv_vocab,embeddings = {},[],[] #Egituren definizioa               \n",
        "with open('/content/glove.txt','rt') as fi: #Ireki embedding dokumentua\n",
        "        cols=fi.readline().split(\" \") #Haren ezaugarriak eskuratu  \n",
        "        vocab_size=int(cols[0])         \n",
        "        embed_dim=int(cols[1])                  \n",
        "        # Hasieratu bi token gehigarri: betegarria (_0_) eta hiztegitik kanpo (_UNK_)\n",
        "        vocab[\"_0_\"]=0                     \n",
        "        vocab[\"_UNK_\"]=1      \n",
        "        inv_vocab.append(\"_0_\")\n",
        "        inv_vocab.append(\"_UNK_\")             \n",
        "        embeddings.append(np.zeros(embed_dim))  # 0 = _0_ betegarriarentzat \n",
        "        embeddings.append(np.zeros(embed_dim))  # 1 = _UNK_\n",
        "        full_content = fi.read().strip().split('\\n')\n",
        "\n",
        "print(\"\\nEmbeddings vocabulary size: %d\" % (vocab_size))\n",
        "print(\"Embeddding dimensions:      %d\" % (embed_dim))\n",
        "\n",
        "for i in range(len(full_content)): \n",
        "    if i+2 >= vocab_size:\n",
        "        break\n",
        "    i_word = full_content[i].split(' ')[0]\n",
        "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
        "    vocab[i_word] = i+2  \n",
        "    inv_vocab.append(i_word)\n",
        "    embeddings.append(i_embeddings)\n",
        "\n",
        "# Numpy array batean gorde (ondoren pytorch tentsore bihurtzeko)\n",
        "embs_npa = np.array(embeddings)\n",
        "\n",
        "def get_word_ids(esaldia, vocab, max_length=200):\n",
        "    tokenak = nlp(str(esaldia))\n",
        "    wids= [ ]\n",
        "    kont=0\n",
        "    for token in tokenak:\n",
        "      if kont == max_length:\n",
        "        break\n",
        "      val = vocab[str(token).lower()] if str(token).lower() in vocab else 1\n",
        "      wids.append(val)\n",
        "      kont+=1\n",
        "        # esaldi luzera konstante bat ezartzeko, betegarria\n",
        "    for i in range(len(wids),max_length):\n",
        "      wids.append(0)\n",
        "    return torch.from_numpy(np.array(wids))"
      ],
      "metadata": {
        "id": "O_adgNElK89j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sintaxi hiztegia definitzea\n",
        "\n",
        "Azpiko kodeak SFN arkitekturak esaldi bakoitzari ezaugarri bektoreak sortzerakoan kontuan hartuko dituen hitz motak definitzen ditu, kasu honetan, Spacy-ren etiketatzaileak bereizi ditzakeen ia guztiak. "
      ],
      "metadata": {
        "id": "_MkNet35Mc72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sintaxHiztegia = { \n",
        "    \"AFX\": 0,\n",
        "    \"CC\": 1,\n",
        "    \"CD\": 2,\n",
        "    \"DT\": 3,\n",
        "    \"EX\": 4,\n",
        "    \"IN\": 5,\n",
        "    \"JJ\": 6,\n",
        "    \"JJR\": 7,\n",
        "    \"JJS\": 8,\n",
        "    \"LS\": 9,\n",
        "    \"MD\": 10,\n",
        "    \"NN\": 11,\n",
        "    \"NNS\": 11,\n",
        "    \"NNP\": 12,\n",
        "    \"NNPS\": 13,\n",
        "    \"PDT\": 14,\n",
        "    \"POS\": 15,\n",
        "    \"PRP\": 16,\n",
        "    \"PRP$\": 17,\n",
        "    \"RB\": 18,\n",
        "    \"RBR\": 19,\n",
        "    \"RP\": 20,\n",
        "    \"TO\": 21,\n",
        "    \"UH\": 22,\n",
        "    \"VB\": 23,\n",
        "    \"VBD\": 24,\n",
        "    \"VBG\": 25,\n",
        "    \"VBN\": 26,\n",
        "    \"VBP\": 27,\n",
        "    \"VBZ\": 28,\n",
        "    \"WDT\": 29,\n",
        "    \"WP\": 30,\n",
        "    \"WP$\": 31,\n",
        "    \"WRB\": 32,\n",
        "    \"ADD\": 33,\n",
        "    \"BES\": 34,\n",
        "    \"HVS\": 35\n",
        "}"
      ],
      "metadata": {
        "id": "uv8mHHZcMyPG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Historia hiztegia definitzea\n",
        "\n",
        "Beheko kode honek aldiz, esaldietako historiak sortzerako orduan kontuan hartuko diren hitz motak definitzen ditu. Kasu honetan, esaldi batean garrantzitsuenak izaten diren hitz motak direla eta, izen arruntak, izen arrunt singularrak, izen arrunt pluralak, izen bereziak eta aditzak hartuko dira kontuan. "
      ],
      "metadata": {
        "id": "jmKRFYWDM0E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kontuanHartuMotak = [\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"VB\"]"
      ],
      "metadata": {
        "id": "5xtOLhwVNO3n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esaldien historiak landuko dituzten azpiprogramak definitzea\n",
        "\n",
        "Edozein idazlanetan, dokumentuak aurrera egin ahala irakurleak gero eta informazio gehiago jasotzen du. Laburpenak egiterakoan oso garrantzitsua izaten da aurreko esaldietan esan izan dena gogoratzea, informazio errepikakorra atzeman eta baztertzeko. Hau da historien lana, uneko esaldiaren aurreko esaldietan esandakoa gogoratzea. Zehazki, aurrez definitutako historia hiztegiko hitz motetako hitzak bakarrik gogoratuko dira, hitz mota bakoitzetik histTamaina adina hitz ezberdin. Adibidez, gehien errepikatu diren histTamina=3 aditzak gogoratuko dira eta horiek agertu diren aldi kopurua ere bai. \n",
        "\n",
        "Prozedura hau gehitzearen arrazoiak zabalago azaldu dira txostenean."
      ],
      "metadata": {
        "id": "wHnQ5luor7j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esaldi historia hiztegiak sortzen dituen azpifuntzioa.\n",
        "def sortuEsaldiHistoria (kontuanHartuMotak):\n",
        "  historia = { } #Lehen hiztegia.\n",
        "  for i in kontuanHartuMotak: #Hiztegiaren barnean hitz mota bakoitzeko beste hiztegi huts bat.\n",
        "    historia[i]={ }\n",
        "  return(historia)\n",
        "#Historien eguneraketa egiten duen funtzioa esaldiko.\n",
        "def eguneratuHistoria (orainArtekoHistoria,nlpHitzak,kontuanHartuMotak):\n",
        "  esaldiHistoria = sortuEsaldiHistoria(kontuanHartuMotak) #Uneko esaldiaren historia soilik. Hiztegia hasieratu.\n",
        "  for i in nlpHitzak: #Hitz bakoitzeko egin\n",
        "    hitzMota = str(i.tag_) #Haren mota jakin\n",
        "    hitza=str(i) #Haren testua eskuratu\n",
        "    if hitzMota not in kontuanHartuMotak: #Ez bada kontuan hartu behar den hitz mota batekoa (hiztegi hau dokumentu osoko historiarena da, globala)\n",
        "      continue #Ez egin jaramonik\n",
        "    if hitzMota in orainArtekoHistoria: #Hitz mota kontuan hartu behar abda eta jada hasieratuta badago\n",
        "      if hitza in orainArtekoHistoria[hitzMota]: #Hitz zehatz hau jada lehenago agertu bada\n",
        "        orainArtekoHistoria[hitzMota][hitza]+=1 #Eguneratu bere agerpen kopurua\n",
        "      else: #Bestela\n",
        "        orainArtekoHistoria[hitzMota][hitza]=1 #Hasieratu bere agerpen kopurua\n",
        "    else: #Bestela\n",
        "      orainArtekoHistoria[hitzMota]={hitza: 1} #Hitz mota hasieratu, uneko hitza lehen aldiz agertu da\n",
        "\n",
        "    if hitza not in esaldiHistoria[hitzMota]: #Hitza lehen aldiz agertu bada esaldian\n",
        "      esaldiHistoria[hitzMota][hitza]=1 #Eguneratu kopurua\n",
        "    else:\n",
        "      esaldiHistoria[hitzMota][hitza]+=1 #Bestela areagotu kopurua\n",
        "  return(orainArtekoHistoria,esaldiHistoria) #Itzuli historia globala eta esaldiarena\n",
        "\n",
        "#Historiak tratatzeko funtzio orokorra\n",
        "def gordeHistoria (esaldiHistoria,orainArtekoHistoria,kopurua,kontuanHartuMotak):\n",
        "  tentsorea = [0]*kopurua*len(kontuanHartuMotak) #Hasieratu historia tentsorea. Bere tamaina historia tamainak (kopurua) eta kontuan hartuko diren mota kopuruak baldintzatuko du.\n",
        "  kont=0 \n",
        "  for i in esaldiHistoria: #Uneko esaldiaren historia aztertu:\n",
        "    esaldiHistoria[i] = {k: esaldiHistoria[i].get(k,0) / orainArtekoHistoria[i][k] for k in set(esaldiHistoria[i])} #Atera maiztasunak, uneko esaldian hitz bakoitzak izan duen agerpena globalean izan duenarekiko proportzioa.\n",
        "    ordenatuta = sorted(esaldiHistoria[i].values(), reverse=True)[0:kopurua] #Ordenatu handienetik txikienera.\n",
        "    if (len(ordenatuta) < kopurua): #Konponketa: agian esaldia motzegiak badira eta hitz nahikoa ez baditu.\n",
        "      ordenatuta.extend([0]*(kopurua-len(ordenatuta)))\n",
        "    tentsorea[kont*kopurua:(1+kont)*(kopurua)]=ordenatuta #Txertatu tentsorean maiztasun ohi baino handiagoko (globalarekiko) hitzen maiztasunak. \n",
        "    kont+=1 #Segi hurrengo hitz motarekin\n",
        "  return(tentsorea)"
      ],
      "metadata": {
        "id": "hTZ30ULnsA5e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ezaugarri bektoreak sortzeko azpiprogramen definizioa\n",
        "\n",
        "Azpiprograma hauek esaldietatik ezaugarri nabarmenak ateratzeaz arduratuko dira, ondoren ezaugarri bektoreak sortzeko. Kontuan hartu diren ezaugarriak honakoak dira:\n",
        "- Lehen azpiprogramak, esaldiKarAzterketa, ondorengo datuak biltzen ditu:\n",
        "  - Digitu kopurua. Esaldi batean dagoen zenbakizko karaktere kopurua (0tik hasi eta 9rainokoak). Normalean zenbaki asko dituen esaldia ez da garrantzitsua izan ohi, baina zenbaki batzuk datu garrantzitsuak izan daitezke.\n",
        "  - Parentesi, giltza eta kortxete kopurua. Esaldi batean dagoen parentesi eta antzeko puntuazio-ikur kopurua. Parentesi askoko esaldiak eduki ugari izan ohi du, parentesi barnekoa ez oso garrantzisua, baina parentesia behar izateko idazten den testua garrantzitsua izan ohi da, xehetasun gehigarriak behar baititu. \n",
        "  - Kakots kopurua. Esaldi batean dagoen kakots eta antzeko ikur kopurua. Goitizenak adierazteko erabiltzen dira askotan, beraz, izenak ere agertuko dira haien ondoan zenbaitetan, interesgarria izan daiteke sistemarentzat. \n",
        "\n",
        "- Esaldi luzera. Esaldi motzegiak edo luzeegiak ez dira laburpenean gehitzeko hautagai onak izaten.\n",
        "\n",
        "- TFIDF indizea. Laburpengintza automatikoan askotan erabilia izan den ezaugarri bat da. Esaldi baten barneko hitzen maiztasuna dokumentuan hitz horiek duten maiztasunarekin alderatuta lortzen den koefizientea da. Maiztasunak zenbat eta desberdinagoak izan, esaldi hori orduan eta apartekoagoa izan ohi da testuaren barnean, hitz ezohiko asko dituelako. \n",
        "- Kokapena. Esaldiak dokumentuaren barnean duen kokapena. Zenbaki huts bat da, esaldia dokumentuaren barneko zenbatgarrena den adierazten duena. Baliagarria izan dakioke sistemari bereizteko dokumentuaren hasiera, dokumentuaren garapena eta dokumentuaren bukaera, testu baten ohiko hiru zatiak.\n",
        "- Sintaxi indizeak. Lehenago definitutako sinatxi hiztegiko hitz mota bakoitzaren maiztasuna testuan. Sistemari aukera emango dio aditz asko, izen asko edo bestelako hitz mota bateko hitz asko agertzen diren esaldiak identifikatzeko. \n",
        "- Aurreko esaldien historia. Aurreko atalean azaldu denaren arabera, aurreko esaldietan gehien agertu diren mota bakoitzeko hitzen maiztasunak. "
      ],
      "metadata": {
        "id": "8HgY7-DGNf0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esaldien karaktereak aztertzeko funtzioa\n",
        "def esaldiKarAzterketa (esaldia):\n",
        "  kop=0\n",
        "  par=0\n",
        "  kakots=0\n",
        "  for j in esaldia: #Esaldiko token bakoitzeko egin\n",
        "    for i in j: #Tokenaren karaktere bakoitzeko egin \n",
        "      if (i == 0 or i == 1 or i == 2 or i == 3 or i == 4 or i == 5 or i == 6 or i == 7 or i == 8 or i == 9): #Begiratu ea digitua den\n",
        "        kop+=1\n",
        "      elif (i == \"(\" or i == \")\" or i == \"[\" or i == \"]\" or i == \"{\" or i == \"}\"): #Begiratu ea parentesi motakoa den\n",
        "        par+=1\n",
        "      elif (i == \"'\" or i == \"'\" or i == \"`\" or i == \"´\"): #Begiratu ea kakots motakoa den\n",
        "        kakots+=1\n",
        "  return(kop,par,kakots) #Itzuli kopuruak\n",
        "#TFIDFa kalkulatzeko funtzio laguntzailea\n",
        "def TFIDF (testua,luzera):\n",
        "  testuHitzak = {} #Testuaren hitzak biltzen dituen hiztegia\n",
        "  for i in testua: #Token bakoitzeko egin\n",
        "    if i not in testuHitzak: #Tokena oraindik ez badago hiztegian gehitu eta kopurua hasieratu\n",
        "      testuHitzak[str(i).lower()]=1/luzera #Larri edo xehe, hitz bera.\n",
        "    else: #Bestela kopurua gehitu\n",
        "      testuHitzak[str(i).lower()]+=1/luzera\n",
        "  return(testuHitzak)\n",
        "#Sintaxiaren ezaugarrientzako funtzio laguntzailea\n",
        "def sintaxia (esal,sintaxHiztegia):\n",
        "  ezaug = [0] * len(sintaxHiztegia) #Hasieraketa\n",
        "  for hitza in esal: #Esaldiko token bakoitzeko\n",
        "    if (hitza.tag_ not in sintaxHiztegia.keys()): #Hitz mota ez badago sintaxi hiztegian aurrera jarraitu\n",
        "      continue\n",
        "    else: #Bestela, dagokion kopuru eguneraketa egin\n",
        "      ezaug[sintaxHiztegia[hitza.tag_]]+=1/len(esal)\n",
        "  return(ezaug)\n",
        "#Esaldi luzera kalkulatu eta karaktere azterketaren kopuruak normalizatzeko funtzioa.\n",
        "def kalkulatuBeheEzaugarriak (esaldia):\n",
        "  luzera=len(str(esaldia))\n",
        "  dig, par, kakots = esaldiKarAzterketa(str(esaldia))\n",
        "  return ([luzera,dig/luzera,par/luzera,kakots/luzera])\n",
        "#Esaldien mailan ezaugarriak kalkulatzen dituen funtzioa\n",
        "def kalkulatuTFIDFetaSintaxia (baliozkoa,dokumentua,sintaxHiztegia,kontuanHartuMotak,histTamainak):\n",
        "  esaldiak = dokumentua #Jada esalditan banatuta dago testua\n",
        "  tfidfa = { }\n",
        "  esaldiena = [ ]\n",
        "  sintaxiarena = [ ]\n",
        "  beheEz = [ ]\n",
        "  kokapenak = [ ]\n",
        "  kont=0\n",
        "  orainArtekoHistoria = { }\n",
        "  historiak = [ ]\n",
        "  for i in esaldiak: #Esaldi bakoitzeko egin\n",
        "    est = nlp(str(i)) #Hura tokenizatu, hitzetan banagarria izateko.\n",
        "    orainArtekoHistoria, esaldiHistoria = eguneratuHistoria (orainArtekoHistoria,est,kontuanHartuMotak) #Esaldiaren historia kalkulatu eta historia globala eguneratu\n",
        "    esTf = TFIDF(est,len(str(i))) #TFIDF indizea kalkulatu esaldian\n",
        "    if (kont in baliozkoa): #Uneko esaldia datu-baseko adibideetan gehitzea erabaki bada egin\n",
        "        historiak.append(gordeHistoria (esaldiHistoria,orainArtekoHistoria,histTamainak,kontuanHartuMotak)) #Historia dagokion zerrendara gehitu. Maiztasun bihurtu.\n",
        "        beheEz.append(kalkulatuBeheEzaugarriak(str(i))) #Karaktere mailako ezaugarriak kalkulatu eta dagokien zerrendara gehitu.\n",
        "        kokapenak.append(kont / len(esaldiak)) #Kokapena dagokion zerrendara gehitu.\n",
        "        esaldiena.append(esTf) #TFIDFa dagokion zerrendara gehitu.\n",
        "        sintaxiarena.append(sintaxia(est,sintaxHiztegia)) #Sintaxi ezaugarriak dagokien zerrendara gehitu.\n",
        "    kont+=1 #Kokapena kontuan hartzeko kontagailua.\n",
        "  tfidfa = {k: tfidfa.get(k,0) / (len(dokumentua)+1) for k in set(tfidfa)} #Dokumentuaren hitz-maiztasunak gordetzen dituen hiztegia, eguneratu uneko esaldiarekin.\n",
        "  return (esaldiena, tfidfa, sintaxiarena, beheEz,kokapenak,historiak)\n",
        "\n",
        "#Ezaugarri tentsoreak sortzen dituen funtzioa, dokumentu bakoitzeko.\n",
        "def sortuEzaugarriTentsoreak (baliozkoa,dokumentua,sintaxHiztegia,kontuanHartuMotak,histTamainak):\n",
        "  esaldienTFIDF, dokumTFIDF, sintaxEsald, beheEzaugarriak, kokapenak, historiak = kalkulatuTFIDFetaSintaxia(baliozkoa,dokumentua,sintaxHiztegia,kontuanHartuMotak,histTamainak) #Ezaugarriak kalkulatu\n",
        "  esaldienTEN = [ ]\n",
        "  kont=0\n",
        "  for i in range(len(dokumentua)): #Esaldi bakoitzeko\n",
        "    if (i not in baliozkoa): #Esaldia datu-basean gehitzeko hautatua zian ez bada jarraitu.\n",
        "        continue\n",
        "    DOt = {k: esaldienTFIDF[kont].get(k, 0) / (dokumTFIDF.get(k, 0)+1) for k in set(esaldienTFIDF[kont]) | set(dokumTFIDF)} #TFIDFa kalkulatu esaldi bakoitzaren maiztasunak dokumentukoekin zatituz. \n",
        "    bbET = sum(DOt.values())/len(DOt) #Horien batez bestekoa kalkulatu.\n",
        "    zerrenda = beheEzaugarriak[kont] + [kokapenak[kont],bbET] + sintaxEsald[kont] + historiak[kont] #Ezaugarri guztiak biltzen dituen zerrenda sortu.\n",
        "    esaldienTEN.append(torch.tensor(np.asarray(zerrenda))) #Zerrenda tentsore bihurtu eta tentsore zerrendara gehitu.\n",
        "    kont+=1\n",
        "  return(esaldienTEN)"
      ],
      "metadata": {
        "id": "9jU2JPdKNlDf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aurreprozesaketa azpiprogramak\n",
        "\n",
        "Azpiko azpiprogramek entrenamendurako eta balidaziorako datu-baseak sortzen dituzte. Datu-base hauek multi_news datu-baseko dokumentuak aztertuz sortu dira, baina ereduak esaldika ikasiko duenez, dokumentuen esaldi bakoitzeko adibide bat sortzen da. Adibide bakoitzak bi zati izango ditu: esaldia embedding-indize gisa ipinita duen tentsorea eta ezaugarri tentsorea, aurreko atalean aipatutako ezaugarriak aurrez konputatuta dituena. Aurreprozesaketa honek entrenamendua azkartzea ere badu helburu bezala. "
      ],
      "metadata": {
        "id": "WZN7XqFFwAGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ROUGE-L indizea kalkulatu emandako esaldia eta laburpenaren artean\n",
        "def esaldiarenEsanguratsutasuna (esaldia,laburpena,rouge):\n",
        "  if (len(str(esaldia)) < 10): #Esaldia laburregia bada, baztertu. \n",
        "    return(0.0)\n",
        "  rougeL = rouge.get_scores(str(esaldia),laburpena)\n",
        "  return(rougeL[0][\"rouge-l\"][\"f\"])\n",
        "\n",
        "#Partizio baten gainean aurreprozesaketa egiten duen funtzioa.\n",
        "def partizioaAP (partizioa,part_izena,rouge,partition,labels,dok_kop,vocab,sintaxHiztegia,kontuanHartuMotak,histTamainak):\n",
        "  indizea=0\n",
        "  for i in tqdm(partizioa): #Partizioko dokumentu bakoitzeko egin:\n",
        "    try:\n",
        "        if (indizea == dok_kop): #Aztertzea nahi den dokumentu kopuru maximoa.\n",
        "          break #Hartara iristean gelditu exekuzioa.\n",
        "        if (len(i[\"summary\"]) < 1): #Laburpenik ez badago, jarraitu.\n",
        "          continue\n",
        "        esaldiak = list(nlp(i[\"document\"]).sents) #Testua esaldietan bereizi.\n",
        "        esalK = len(esaldiak)//4 #Adibideak sortzeko uneko dokumentutik hartu nahi den esaldi kopurua. Esaldi bakoitzaka dibide bat sortuko du. Kasu honetan esaldien %25a (berez %50, ikus aurrerago) baino ez da baliatuko adibideak egiteko.\n",
        "        esEsan = [] #Esaldien ROUGE-L indizeak gordetzeko zerrenda\n",
        "        indizea2=0\n",
        "        for j in esaldiak: #Esaldi bakoitzeko\n",
        "          une_izena = \"ID\" + str(indizea) + \"_\" + str(indizea2) #Adibidea gordetzeko bide-izenerako.\n",
        "          esEsan.append((esaldiarenEsanguratsutasuna(j,i[\"summary\"],rouge),une_izena,indizea2)) #Kalkulatu haren ROUGE-L indizea.\n",
        "          indizea2+=1 \n",
        "        ordenatuta = sorted(esEsan, key=lambda tup: tup[0],reverse=True) #Ordenatu zerrenda ROUGE-L altuenetik baxuenera.\n",
        "        if (len(ordenatuta) >= esalK*2): #Ziurtatu esaldi nahikoa dagoela.\n",
        "          onartuakOnak = [x[2] for x in ordenatuta[0:esalK]] #ROUGE-L indize altuenekoak hautatu. Onenak izan beharko luketenak laburpena egiteko.\n",
        "          onartuakTxarrak = [x[2] for x in ordenatuta[len(ordenatuta)-esalK:len(ordenatuta)]] #ROUGE-L indize baxuenekoak hautatu. Txarrenak izan beharko luketenak laburpena egiteko. \n",
        "          ezaugTentsoreak = sortuEzaugarriTentsoreak (onartuakOnak+onartuakTxarrak,esaldiak,sintaxHiztegia,kontuanHartuMotak,histTamainak) #Ezaugarri tentsoreak sortu esaldi hautatu guztientzat.\n",
        "          for j in range(esalK*2): #Esaldi hautatu guztientzat egin:\n",
        "            if (j < esalK): #Onenen artean badago\n",
        "              ind=ordenatuta[j][1]\n",
        "              labels[ind]=1 #Haren labela 1 izango da, laburpenean gehitu.\n",
        "            else: #Bestela\n",
        "              ind=ordenatuta[len(ordenatuta)-j][1]\n",
        "              labels[ind]=0 #Haren labela 0 izango da, laburpenean ez gehitu.\n",
        "            nuk=get_word_ids(esaldiak[int(ind.split(\"_\")[1])],vocab) #Embedding tentsoreak kalkulatu LSTMarentzat.\n",
        "            torch.save(nuk,ind+'.pt') #Gorde embedding tentsoreak (indizeak)\n",
        "            torch.save(ezaugTentsoreak[j],ind+'Ezaug.pt') #Gorde ezaugarri tentsoreak\n",
        "            partition[part_izena].append(ind) #Partizioa eguneratu\n",
        "        indizea+=1\n",
        "    except: #Edozein arazo bada, jarraitu hurrengo dokumentuarekin.\n",
        "        continue\n",
        "  return (partition, labels)\n",
        "\n",
        "#Aurreprozesamendua gidatzen duen funtzio nagusia.\n",
        "def aurreprozesamendua (bide_izena,zenbat_dok_corpusean,datuak,dok_kop,vocab,sintaxHiztegia,kontuanHartuMotak,histTamainak):\n",
        "  indizea= 0 #Hasieraketak\n",
        "  partition = {\"train\": [],\n",
        "               \"validation\": []\n",
        "               }\n",
        "  labels = { }\n",
        "  os.chdir(bide_izena+ \"esaldiak/\") #Adibideak gordeko diren karpeta zehaztu.\n",
        "  rouge = Rouge() #Rouge objektua sortu.\n",
        "  partition, labels = partizioaAP(datuak[\"train\"],\"train\",rouge,partition,labels,dok_kop,vocab,sintaxHiztegia,kontuanHartuMotak,histTamainak) #Entrenamenduko adibideak sortu.\n",
        "  partition, labels = partizioaAP(datuak[\"validation\"],\"validation\",rouge,partition,labels,dok_kop//8,vocab,sintaxHiztegia,kontuanHartuMotak,histTamainak) #Balidazioko adibideak sortu.\n",
        "  os.chdir(\"../../\")\n",
        "  return partition, labels\n",
        "\n",
        "bide_izena=\"/content/\"\n",
        "\n",
        "histTamainak=3 #Historia tamaina 3koa izango da, hitz mota bakoitzeko hiru hitz ugarienak oroitu.\n",
        "\n",
        "partition, labels = aurreprozesamendua(bide_izena,22000,dataset,15000,vocab,sintaxHiztegia,kontuanHartuMotak,histTamainak)"
      ],
      "metadata": {
        "id": "aizXkQhRwql7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aurreprozesaketa jada egina dago, koaderno honetan exekuzioak egiten jarraitzeko ondorengo aurreprozesatutako datu-basea erabili dezakezu:"
      ],
      "metadata": {
        "id": "h9sndtsqwy8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/15000koa/partition.json\") as f_in:\n",
        "        partition= json.load(f_in)\n",
        "with open(\"/content/15000koa/labels.json\") as f_in:\n",
        "        labels= json.load(f_in)"
      ],
      "metadata": {
        "id": "A-BkqX08xC-f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ereduaren definizioa\n",
        "\n",
        "Esan bezala, erauzketa bidezko laburpengintza egiteko SFN arkitektura erabiliko dugu. "
      ],
      "metadata": {
        "id": "Xl-sw4TGxEsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SFN(torch.nn.Module):\n",
        "  def __init__ (self,embeddings,hidden_dim,num_layers,conc_dim,droP,PoSMax,HisTam):\n",
        "    '''''\n",
        "    Sarrerak:\n",
        "\n",
        "    Embeddingak: VOCAB sortzen duen funtziotik ekarriak.\n",
        "    Hidden_dim: LSTMaren geruza ezkutu kopurua.\n",
        "    Num_layers: LSTMaren geruza kopurua.\n",
        "    Conc_dim: LSTMaren irteera eta ezaugarrien aztertzailearena zenbateko dimentsioko egitura batean gorde nahi den.\n",
        "    droP: Dropout probabilitatea.\n",
        "    PoSMax: Sintaxian kontuan hartu den hitz mota kopurua\n",
        "    HisTam: Historia zerrenda baten tamaina (kontuan hartu diren hitz motak * bakoitzetik zenbat)\n",
        "\n",
        "    '''''\n",
        "    super().__init__()\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.embeddings = torch.nn.Embedding.from_pretrained(torch.from_numpy(embeddings).to(device).float(),padding_idx=0)\n",
        "    self.embSize=len(embeddings[0])\n",
        "    self.lstm = torch.nn.LSTM(self.embSize,hidden_dim,num_layers) #LSTMa\n",
        "    self.linear1 = torch.nn.Linear(hidden_dim,3*conc_dim//4) #Lehen geruza lineala\n",
        "    self.linear2out = 6 + PoSMax + HisTam #Ezaugarri geruzaren irteera nolakoa den kalkulatu\n",
        "    self.linear2 = torch.nn.Linear(self.linear2out,conc_dim//4) #Ezaugarri geruzari dagokion irteera\n",
        "    self.linear3 = torch.nn.Linear(conc_dim,conc_dim) #Hirugarren geruzan lineala\n",
        "    self.linear4 = torch.nn.Linear(conc_dim,1) #Laugarrena\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "    self.dropout = torch.nn.Dropout(p=droP)\n",
        "\n",
        "    torch.nn.init.kaiming_uniform_(self.linear1.weight) #Lehen geruzaren hasieraketa.  \n",
        "    torch.nn.init.kaiming_uniform_(self.linear2.weight) #Bigarren geruzaren hasieraketa.\n",
        "    torch.nn.init.kaiming_uniform_(self.linear3.weight) #Hirugarren geruzaren hasieraketa.\n",
        "    \n",
        "    self.normalization1 = torch.nn.BatchNorm1d(3*conc_dim//4) #Sorta normalizazioak\n",
        "    self.normalization2 = torch.nn.BatchNorm1d(conc_dim//4)\n",
        "    self.normalization3 = torch.nn.BatchNorm1d(conc_dim)\n",
        "\n",
        "  def forward (self,embeddingak,ezaugarriak):\n",
        "    '''''\n",
        "    Sarrerak: Embedding tentsorea eta ezaugarri tentsorea. \n",
        "    Irteera: Esaldia laburpenean zenbateko probabilitateaz gehitu behar den (0 ez gehitu, 1 gehitu)\n",
        "    '''''\n",
        "    enbedak = self.embeddings(embeddingak) #Lehenengo embedding-ak eskuratu\n",
        "    all_hidden,_ = self.lstm(enbedak) #LSTMaren egoera-ezkutu guztiak eskuratu.\n",
        "    hidden_max = all_hidden.max(1).values #haien maxpooling-a egin.\n",
        "    irteeraLSTM = self.linear1(hidden_max) #LSTMaren irteera geruza dentso batetik pasa.\n",
        "    irteeraLSTM = self.normalization1(irteeraLSTM) #Hura normalizatu.\n",
        "    irteeraLSTM = self.relu(irteeraLSTM) #Aktibazio-funtzioa aplikatu.\n",
        "    irteeraEZAUG = self.linear2(ezaugarriak.to(torch.float32)) #Ezaugarriak geruza dentso batetik pasa.\n",
        "    irteeraEZAUG = self.normalization2(irteeraEZAUG) #Normalizatu.\n",
        "    irteeraEZAUG = self.relu(irteeraEZAUG) #Aktibazio-funtzioa aplikatu.\n",
        "    konkat = torch.cat((irteeraLSTM,irteeraEZAUG),dim=1) #LSTMaren irteera eta ezaugarriena kateatu.\n",
        "    irteeraESAL = self.linear3(konkat) #Kateaketa hori geruza dentso batera pasa.\n",
        "    irteeraESAL = self.normalization3(irteeraESAL) #Normalizatu.\n",
        "    irteeraESAL = self.relu(irteeraESAL) #Aktibazio-funtzioa aplikatu.\n",
        "    irteeraESAL = self.dropout(irteeraESAL) #Diluzioa aplikatu.\n",
        "    irteeraESAL = self.linear4(irteeraESAL) #Azken geruza dentso batera pasa.\n",
        "    irteeraESAL = self.sigmoid(irteeraESAL) #Sigmoidea aplikatu irteera lortzeko.\n",
        "    return(torch.squeeze(irteeraESAL,1))"
      ],
      "metadata": {
        "id": "9Rzh4DZuxW21"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datu kargatzaileen definizioa\n",
        "\n",
        "Datu kargatzaileak edo dataloaderrak definitzen dira azpian."
      ],
      "metadata": {
        "id": "H6EdbvyqxhTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Datu-basea kargatzeko klasea\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, list_IDs, labels,bide_izena):\n",
        "        'Initialization'\n",
        "        self.labels = labels #Labelak\n",
        "        self.list_IDs = list_IDs #Fitxategien IDak dituen egitura. Partizioa.\n",
        "        self.bide_izena = bide_izena #Fitxategiak gordetzea nahi den karpetaren helbidea.\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        #f = open('data/' + ID + '.pt')\n",
        "        #X = f.read()\n",
        "        #f.close()\n",
        "        X = torch.load(self.bide_izena + ID + '.pt')\n",
        "        Z = torch.load(self.bide_izena + ID + 'Ezaug.pt')\n",
        "        y = self.labels[ID]\n",
        "\n",
        "        return X, Z, y\n",
        "  def my_collate_fn(data):\n",
        "    # TODO: Implement your function\n",
        "    # But I guess in your case it should be:\n",
        "    return torch.cat(data,dim=0)"
      ],
      "metadata": {
        "id": "7pGfNK9TxoIL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamendu prozeduraren definizioa\n",
        "\n",
        "Ereduaren entrenamendua egiteko gelditze goiztiarra erabili da. Honen bidez, gorde den eredua balidazioko errore baxuenekoa izan dela ziurtatzeaz gain, balidazioa \"pazientzia\" aldiz segidan okertuta ere, entrenamenduak aurrera jarraitu du, okertze txikien ondorioz entrenamendua gelditzea saihestuz. \n",
        "\n",
        "Gainera, \"pausoa\" edo \"step\" motako ikaskuntza-tasaren planifikatzaile bat erabiltzen da, hiru hutsegite jarraian egin ondoren hamarren batera gutxitzen duena ikaskuntza-tasa. Horrela, ereduari doitzeko aukera ematen zaio."
      ],
      "metadata": {
        "id": "iA7T8h2Oxrd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Nolabaiteko asmatze-tasa ematen duen funtzioa. Sailkapen bitarreko problema baten aurrean gaudenez, esaldia laburpenean bai edo ez, asmatutzat kontsideratzen da sigmoidearen irteera 0.5 baino \n",
        "#altuagoa denean eta esaldia laburpenean gehitu behar denean, aurkako kasuan ere bai eta kasu trukatuetan ez. \n",
        "def ebaluatu (iragarpenak,etiketak):\n",
        "    iragarpen_etiketak = iragarpenak > 0.5 #Esandakoa, boolear bektore bihurtu iragarpenak.\n",
        "    alderaketa = torch.abs(iragarpen_etiketak.int() - etiketak) #Egin kenketa labelekin.\n",
        "    asmatuak = torch.sum(alderaketa).cpu() #Okerren (1en, labela eta iragarpena desberdin) batura egin\n",
        "    alderaketa_tam = (alderaketa.size())[0] #Elementu kopurua lortu\n",
        "    asmatze_tasa = asmatuak.numpy()/alderaketa_tam #Hutsegite-tasa kalkulatu\n",
        "    return((1-asmatze_tasa)*100) #1 - Hutsegite-tasa = Asmatze-tasa\n",
        "\n",
        "#Entrenamendua gauzatzen duen funtzioa.\n",
        "def train (partition,labels,ePar,pazientzia):\n",
        "    # CUDA for PyTorch\n",
        "    use_cuda = torch.cuda.is_available() \n",
        "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "    #Ereduaren parametroen zehaztapena.\n",
        "    batch_size = ePar[\"batch_size\"]\n",
        "    lr = ePar[\"lr\"]\n",
        "    hidden_dim = ePar[\"hidden_dim\"]\n",
        "    num_layers = ePar[\"num_layers\"]\n",
        "    conc_dim = ePar[\"conc_dim\"]\n",
        "    sintaxTam = ePar[\"sintaxTam\"]\n",
        "    historiaKontuan = ePar[\"historiaKontuan\"]\n",
        "    histTam = ePar[\"histTam\"]\n",
        "    weight_decay = ePar[\"weight_decay\"]\n",
        "    dropout= ePar[\"dropout\"]\n",
        "\n",
        "    # Bestelako parametroak.\n",
        "    params = {'batch_size': batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 6}\n",
        "\n",
        "    losses = [ ]\n",
        "\n",
        "    # Sorten sortzaileak martxan jarri.\n",
        "    training_set = Dataset(partition['train'], labels,'/content/15000koa/esaldiak/')\n",
        "    training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
        "\n",
        "    validation_set = Dataset(partition['validation'], labels,'/content/15000koa/esaldiak/')\n",
        "    validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
        "\n",
        "\n",
        "    min_loss = 1000000 #Galera minimoa, minimoaren hasieraketa egiteko. Praktika txarra.\n",
        "    epoch = 0 #Lehen aroa.\n",
        "    eredua = SFN(embs_npa,hidden_dim,num_layers,conc_dim,dropout,sintaxTam,histTam*historiaKontuan).to(device) #Ereduaren hasrieraketa.\n",
        "    optimizer = torch.optim.Adam(eredua.parameters(), lr=lr, weight_decay=weight_decay) #Adam optimizatzailearen hasieraketa.\n",
        "    bce_entropy=torch.nn.BCELoss() #Galera irizpidea, entropia gurutzatu bitarra.\n",
        "    \n",
        "    guztira = sum(p.numel() for p in eredua.parameters()) #Parametro kopurua guztira.\n",
        "    entrenagarriak = sum(p.numel() for p in eredua.parameters() if p.requires_grad) #Parametro ikasgarri kopurua.\n",
        "    print(\"----------------------------\")\n",
        "    print(\"\\t\\tSFN eredua\\t\\t\\t\")\n",
        "    print(\"-> Parametro kopurua guztira: \",guztira)\n",
        "    print(\"-> Parametro entrenagarriak: \",entrenagarriak)\n",
        "    print(\"----------------------------\")\n",
        "    \n",
        "    # Aro iterazioa.\n",
        "    epoch=0 #Lehen aroan gaude.\n",
        "    nekea=0 #Ez dago hustegiterik, pazientziari begira. \n",
        "    while (nekea < pazientzia): #Pazientzia agortu arte egin:\n",
        "        print(\"\\n\\n\",epoch,\". AROA. Nekea: \",nekea,\"Ikaskuntza-tasa: \",lr)\n",
        "        # Entrenatu\n",
        "        losses = [ ]\n",
        "        for local_batch, ezaug_batch, local_labels in tqdm(training_generator):\n",
        "            # GPUra transferitu\n",
        "            local_batch, ezaug_batch, local_labels = local_batch.to(device), ezaug_batch.to(device), local_labels.to(device)\n",
        "            y_pred = eredua.forward(local_batch,ezaug_batch)\n",
        "            loss = bce_entropy(y_pred.float(),local_labels.float())\n",
        "            losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(\"\\tEntrenamendu galera: \",(sum(losses)/len(losses)))\n",
        "        # Balidazioa\n",
        "        losses = [ ]\n",
        "        asmatze_tasa=0\n",
        "        with torch.set_grad_enabled(False):\n",
        "            for local_batch, ezaug_batch, local_labels in tqdm(validation_generator):\n",
        "                # GPUra transferitu\n",
        "                local_batch, ezaug_batch, local_labels = local_batch.to(device), ezaug_batch.to(device), local_labels.to(device)\n",
        "                y_pred = eredua.forward(local_batch,ezaug_batch)\n",
        "                loss = bce_entropy(y_pred.float(),local_labels.float())\n",
        "                losses.append(loss.item())\n",
        "                asmatze_tasa+=ebaluatu(y_pred,local_labels) #Asmatze-tasaren kalkulua\n",
        "        if (sum(losses)/len(losses) < min_loss): #Gelditze goiztiarra. Inoizko eredurik onena bada gorde eta pazientzia hasieratu.\n",
        "            nekea=0\n",
        "            min_loss = sum(losses)/(len(losses))\n",
        "            torch.save(eredua.state_dict(), \"/content/laburtzailea.pt\")\n",
        "        else: #Bestela pazientzia areagotu.\n",
        "            nekea+=1\n",
        "            if (nekea % 3 == 0): #pazientzia hiruren multiploa bada:\n",
        "                lr=lr*0.1 #Ikaskuntza-tasa hamarren batera jaitsi.\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "        epoch+=1\n",
        "        print(\"\\tGarapen galera: \",(sum(losses)/len(losses)),\" Asmatze-tasa: %\",asmatze_tasa/len(losses),\" \\n\\n\")"
      ],
      "metadata": {
        "id": "z5JtwugQzWIT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ePar = {\n",
        "    \"batch_size\": 256,\n",
        "    \"lr\": 0.01,\n",
        "    \"hidden_dim\": 128,\n",
        "    \"num_layers\": 3,\n",
        "    \"conc_dim\": 256,\n",
        "    \"luzMax\": 0.2,\n",
        "    \"sintaxTam\": len(sintaxHiztegia),\n",
        "    \"historiaKontuan\": len(kontuanHartuMotak),\n",
        "    \"histTam\": 3,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "pazientzia=30\n",
        "train(partition,labels,ePar,pazientzia) #Hasi entrenamendua."
      ],
      "metadata": {
        "id": "lzebIhc_zddS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test prozeduraren definizioa\n",
        "\n",
        "Ereduaren testa egiteko prozedura definituko dugu. Era horretan ereduak testean duen galera zenbatekoa den ere jakingo dugu. Erroreren bat ematen badu, ziurtatu GPUan exekutatzen ari zarela."
      ],
      "metadata": {
        "id": "_PrVkJcQzbSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testa burutzen duen programa. \n",
        "def test (eredua,partition,labels,batch_size):\n",
        "    # CUDA hasieraketak\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "    batch_size = ePar[\"batch_size\"]\n",
        "\n",
        "    # Parametroak zehaztea\n",
        "    params = {'batch_size': batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 6}\n",
        "\n",
        "    losses = [ ]\n",
        "\n",
        "    # Sorten sortzaileak martxan jarri.\n",
        "    test_set = Dataset(partition['test'], labels,'/content/testa/esaldiak/')\n",
        "    test_generator = torch.utils.data.DataLoader(test_set, **params)\n",
        "\n",
        "    bce_entropy=torch.nn.BCELoss()\n",
        "    asmatze_tasa=0\n",
        "    with torch.set_grad_enabled(False): #Gradiente jaitsiera gabe.\n",
        "        for local_batch, ezaug_batch, local_labels in tqdm(test_generator):\n",
        "            local_batch, ezaug_batch, local_labels = local_batch.to(device), ezaug_batch.to(device), local_labels.to(device)\n",
        "            y_pred = eredua.forward(local_batch,ezaug_batch)\n",
        "            loss = bce_entropy(y_pred.float(),local_labels.float())\n",
        "            losses.append(loss.item())\n",
        "            asmatze_tasa+=ebaluatu(y_pred,local_labels)\n",
        "    print(\"Test galera: \",(sum(losses)/len(losses)),\" Asmatze-tasa: %\",asmatze_tasa/len(losses),\" \\n\\n\")"
      ],
      "metadata": {
        "id": "Z8TgYkS86Gx7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test partizioa eta labelak kargatuko ditugu, aurretik kargatu diren fitxategietan ez baitaude, segurtasun arrazoiengatik. Jada entrenatutako eredu bat eskuragarri dago, testa haren gainean egiteko."
      ],
      "metadata": {
        "id": "8zUCwjGBL8X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/testa/partition.json\") as f_in:\n",
        "        partition= json.load(f_in)\n",
        "with open(\"/content/testa/labels.json\") as f_in:\n",
        "        labels= json.load(f_in)"
      ],
      "metadata": {
        "id": "kgneiusqLmbJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ePar = { #Hiperparametroak zehaztea\n",
        "    \"batch_size\": 256,\n",
        "    \"lr\": 0.01,\n",
        "    \"hidden_dim\": 128,\n",
        "    \"num_layers\": 3,\n",
        "    \"conc_dim\": 256,\n",
        "    \"luzMax\": 0.2,\n",
        "    \"sintaxTam\": len(sintaxHiztegia),\n",
        "    \"historiaKontuan\": len(kontuanHartuMotak),\n",
        "    \"histTam\": 3,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "#Hiperparametroak kokatzea\n",
        "batch_size = ePar[\"batch_size\"]\n",
        "lr = ePar[\"lr\"]\n",
        "hidden_dim = ePar[\"hidden_dim\"]\n",
        "num_layers = ePar[\"num_layers\"]\n",
        "conc_dim = ePar[\"conc_dim\"]\n",
        "sintaxTam = ePar[\"sintaxTam\"]\n",
        "historiaKontuan = ePar[\"historiaKontuan\"]\n",
        "histTam = ePar[\"histTam\"]\n",
        "weight_decay = ePar[\"weight_decay\"]\n",
        "dropout= ePar[\"dropout\"]\n",
        "\n",
        "eredua = SFN(embs_npa,hidden_dim,num_layers,conc_dim,dropout,sintaxTam,histTam*historiaKontuan).to(device) #Eredua definitu, GPUra bidali\n",
        "eredua.load_state_dict(torch.load(\"/content/laburtzailea.pk\")) #Eredua kargatu\n",
        "eredua.eval() #Testa egingo dela abisatu ereduari.\n",
        "test (eredua,partition,labels,batch_size) #Testa egin."
      ],
      "metadata": {
        "id": "g_3wxqTz6Oz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laburpenak sortzeko azpiprogramaren definizioa\n",
        "\n",
        "Jada eredua entrenatuta, orain hura erabilita laburpenak sortuko dituen azpiprograma bat behar dugu. Azpiprograma honek eredua erabiliko du testu bateko esaldirik garrantzitsuenak zeintzuk diren jakiteko eta ostera, eskatzen zaion laburpen luzeraren arabera, esaldi gehiago edo gutxiago gehituko ditu azken laburpenera. "
      ],
      "metadata": {
        "id": "yzK17nrB6QaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Laburpenak sortzeko funtzioa.\n",
        "def sortuLaburpena (dokumentua,eredua,luz_max,embeddingak,sintaxiGakoak,kontuanHartuMotak,histTamainak):\n",
        "  #Hasieraketak  \n",
        "  embeddi = [ ]\n",
        "  esaldi_prob = [ ]\n",
        "  luz = 0\n",
        "  kont=0\n",
        "  laburpenLuz=0\n",
        "  onartuak = [ ]\n",
        "\n",
        "  esaldiak = list(nlp(dokumentua).sents) #Dokumentua esaldietan banatu.\n",
        "  ezaugarriak = sortuEzaugarriTentsoreak (list(range(len(esaldiak))),dokumentua,sintaxiGakoak,kontuanHartuMotak,histTamainak) #Ezaugarri tentsoreak konputatu.\n",
        "  for i in range(len(ezaugarriak)): #Ezaugarri tentsoreen egokitzapenak.\n",
        "    ezaugarriak[i]=torch.unsqueeze(ezaugarriak[i],0)\n",
        "  for i in esaldiak: #Berdina embedding tentsoreekin, behin horiek eskuratuta.\n",
        "    embeddi.append(torch.unsqueeze(get_word_ids(i,vocab),0))\n",
        "  for i in range(len(esaldiak)): #Esaldi bakoitzeko egin:\n",
        "      esaldi_prob.append(eredua.forward(embeddi[i].to(device),ezaugarriak[i].to(device))) #Ereduari bidali eta haren iragarpena jaso.\n",
        "  ordenatuta = sorted(enumerate(esaldi_prob), key=lambda tup: tup[1],reverse=True) #Ordenatu ereduaren iritziz probableenak diren esaldien arabera.\n",
        "  while (laburpenLuz/len(dokumentua) < luz_max): #Luzera maximora heldu arte esaldiak gehitzen joan.\n",
        "    onartuak.append((str(esaldiak[ordenatuta[kont][0]]),ordenatuta[kont][0])) #Onartuetara gehitu.\n",
        "    laburpenLuz=laburpenLuz+len(str(esaldiak[ordenatuta[kont][0]])) #Laburpen luzera eguneratu.\n",
        "    kont+=1\n",
        "  onartuak = sorted(onartuak, key=lambda tup: tup[1]) #Azken ordenaketa, testuan zuten ordenaren arabera ipintzeko esaldiak eta ez probableenetik probabilitate guxtienekora.\n",
        "  laburpena = ''.join([i[0] for i in onartuak]) #Laburpena egin.\n",
        "  return(laburpena)\n"
      ],
      "metadata": {
        "id": "1sR8gZfb6t-F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proba dezagun haren zuzentasuna kode honekin."
      ],
      "metadata": {
        "id": "AnVAEMni608O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dokumentua = \"The battle of Adys (or Adis) took place in late 255 BC during the First Punic War between a Carthaginian army jointly commanded by Bostar, Hamilcar and Hasdrubal and a Roman army led by Marcus Atilius Regulus.[note 1] Earlier in the year, the new Roman navy established naval superiority and used this advantage to invade the Carthaginian homeland, which roughly aligned with modern Tunisia in North Africa. After landing on the Cape Bon Peninsula and conducting a successful campaign, the fleet returned to Sicily, leaving Regulus with 15,500 men to hold the lodgement in Africa over the winter.Instead of holding his position, Regulus advanced towards the Carthaginian capital, Carthage. The Carthaginian army established itself on a rocky hill near Adys (modern Uthina) where Regulus was besieging the town. Regulus had his forces execute a night march to launch twin dawn assaults on the Carthaginians' fortified hilltop camp. One part of this force was repulsed and pursued down the hill. The other part then charged the pursuing Carthaginians in the rear and routed them in turn. At this the Carthaginians remaining in the camp panicked and fled.The Romans advanced to and captured Tunis, only 16 kilometres (10 mi) from Carthage. Despairing, the Carthaginians sued for peace. The terms offered by Regulus were so harsh that Carthage resolved to fight on. A few months later, at the battle of the Bagradas River (battle of Tunis), Regulus was defeated and his army all but wiped out. The war continued for a further 14 years.\"\n",
        "\n",
        "luz_max = 0.2\n",
        "\n",
        "eredua = SFN(embs_npa,ePar[\"hidden_dim\"],ePar[\"num_layers\"],ePar[\"conc_dim\"],ePar[\"dropout\"],ePar[\"sintaxTam\"],ePar[\"histTam\"]*ePar[\"historiaKontuan\"])\n",
        "eredua.load_state_dict(torch.load(\"/content/laburtzailea.pk\"))\n",
        "eredua.eval()\n",
        "eredua.to(device)\n",
        "\n",
        "laburpena = sortuLaburpena(dokumentua,eredua,luz_max,vocab,sintaxHiztegia,kontuanHartuMotak,ePar[\"histTam\"])   \n",
        "print(laburpena)"
      ],
      "metadata": {
        "id": "vX2-GGIn6zBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE-2 analisia\n",
        "\n",
        "Sistemak sortzen dituen laburpenen kalitatea neurtzeko irizpide automatiko bat ROUGE-2 indizea da, txostenean hobeto azaldua. Sistemari test partizioko testuen artetik lehen ehunak aukeratu eta haien laburpenak sortzeko eskatuko diogu. Ondoren, ROUGE-2 irizpidea erabiliz benetako laburpenkin alderatuko ditugu."
      ],
      "metadata": {
        "id": "ZiV_G6Eg695g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ROUGE-2 analisia egiteko gai den programa. \n",
        "def rougeAnalisia (eredua,test_datuak,zenbat,rouge,parametroak):\n",
        "    jsona = { } #Hasieraketak.\n",
        "    f1ak = [ ]\n",
        "    estaldurak = [ ]\n",
        "    zehaztasunak = [ ]\n",
        "    if (len(test_datuak) > zenbat): #Test partizioko zenbat dokumenturekin egin nahi den azterketa zehaztea. \n",
        "        itak=range(zenbat) #Zenbaki segida eskuratu.\n",
        "    else: #Erroreak saihesteko, eskatutako dokumentu kopurua dagoena\n",
        "        range(len(test_datuak)) \n",
        "    for i in tqdm(itak): #Dokumentu hautatu bakoitzeko egin:\n",
        "        dokumentua=test_datuak[i][\"document\"] #Artikulu orijinala eskuratu.\n",
        "        laburpena=test_datuak[i][\"summary\"] #Jatorrizko laburpena eskuratu.\n",
        "        iragarria=sortuLaburpena (dokumentua,eredua,parametroak[\"luz_max\"],parametroak[\"embeddingak\"],parametroak[\"sintaxiGakoak\"],parametroak[\"kontuanHartuMotak\"],parametroak[\"histTamainak\"]) #Laburpena sortu.\n",
        "        rougeak = rouge.get_scores(iragarria,laburpena)[0][\"rouge-2\"] #Rouge-2a kalkulatu\n",
        "        f1ak.append(rougeak[\"f\"]) #F1a gorde\n",
        "        estaldurak.append(rougeak[\"r\"]) #Recall-a gorde\n",
        "        zehaztasunak.append(rougeak[\"p\"]) #Precision-a gorde\n",
        "        jsona[str(i)]=[i,iragarria,laburpena,rougeak[\"f\"],rougeak[\"r\"],rougeak[\"p\"]] #Denak bildu zerrenda batean\n",
        "    f1bb = sum(f1ak)/len(f1ak) #Batez bestekoak atera F1etan.\n",
        "    estaldurakbb = sum(estaldurak) / len(estaldurak) #Berdina recall-etan.\n",
        "    zehaztasunakbb = sum(zehaztasunak) / len(zehaztasunak) #Berdina precision-etan.\n",
        "    json_string = json.dumps(jsona) #Gorde dokumentu guztien datuak fitxategi batean.\n",
        "    with open('/content/rougeA.json', 'w') as f:\n",
        "        json.dump(json_string, f)\n",
        "    return ([f1bb,estaldurakbb,zehaztasunakbb],f1ak, estaldurak, zehaztasunak) #Itzuli ROUGE-2aren datuak soilik. "
      ],
      "metadata": {
        "id": "GIR04W2X7ZMu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ePar = { #Hiperparametroak zehaztu\n",
        "    \"batch_size\": 256,\n",
        "    \"lr\": 0.01,\n",
        "    \"hidden_dim\": 128,\n",
        "    \"num_layers\": 3,\n",
        "    \"conc_dim\": 256,\n",
        "    \"luzMax\": 0.2,\n",
        "    \"sintaxTam\": len(sintaxHiztegia),\n",
        "    \"historiaKontuan\": len(kontuanHartuMotak),\n",
        "    \"histTam\": 3,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "#Hiperparametroak kokatu\n",
        "batch_size = ePar[\"batch_size\"]\n",
        "lr = ePar[\"lr\"]\n",
        "hidden_dim = ePar[\"hidden_dim\"]\n",
        "num_layers = ePar[\"num_layers\"]\n",
        "conc_dim = ePar[\"conc_dim\"]\n",
        "sintaxTam = ePar[\"sintaxTam\"]\n",
        "historiaKontuan = ePar[\"historiaKontuan\"]\n",
        "histTam = ePar[\"histTam\"]\n",
        "weight_decay = ePar[\"weight_decay\"]\n",
        "dropout= ePar[\"dropout\"]\n",
        "#Parametro gehigarriak laburtzailearentzat\n",
        "parametroak = {\n",
        "    \"luz_max\": 0.2,\n",
        "    \"embeddingak\": vocab,\n",
        "    \"sintaxiGakoak\": sintaxHiztegia,\n",
        "    \"kontuanHartuMotak\": kontuanHartuMotak,\n",
        "    \"histTamainak\": 3\n",
        "}\n",
        "\n",
        "eredua = SFN(embs_npa,hidden_dim,num_layers,conc_dim,dropout,sintaxTam,histTam*historiaKontuan).to(device) #Eredu definitu, GPUra bidali.\n",
        "eredua.load_state_dict(torch.load(\"/content/laburtzailea.pk\")) #Eredua kargatu.\n",
        "eredua.eval() #Testa egingo dela abisatu.\n",
        "rouge = Rouge() #ROUGE objektua hasieratu.\n",
        "bbak, f1ak, estaldurak, zehaztasunak = rougeAnalisia(eredua,dataset[\"test\"],30,rouge,parametroak) #Azterketa egin.\n",
        "print(\"F1: \",bbak[0],\" Estaldura: \",bbak[1],\" Zehaztasuna: \",bbak[2])"
      ],
      "metadata": {
        "id": "T0HNdSH17nMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstrakzio bidezko laburpengintza\n",
        "\n",
        "Abstrakzio bidezko laburpengintza erauzketa bidezko laburpengintzaren eboluzio bezala interpretatu daiteke. Bere helburua laburpen naturalago eta kohesionatuagoak sortzea da, gizakiek egiten duten era imitatzen saiatuz. Hartarako, esaldiak bere horretan testutik erauzi beharrean, testuaren esanahia ulertu behar dute eta ondoren hura ahalik eta laburren azaldu, esaldi berriak sortuz. \n",
        "\n",
        "Laburpengintza mota hau konplexuagoa da erauzketa bidezkoa baino eta ikaskuntza prozesu sakonagoak eskatzen ditu. Horri aurre egiteko T5 eredu aurrentrenatua erabiliko dugu, konputazio gaitasun handiegia eskatuko lukeelako guk bat entrenatzea. Atal honetan transformer arkitekturan oinarritutako eredu hau nola findu erakusten da, gure atazara egokitu dadin."
      ],
      "metadata": {
        "id": "CfR9cWnRRrFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beharrezko fitxategiak jaistea\n",
        "\n",
        "Azpian atal honetan exekuzioak azkartzeko erabiliko ditugun fitxategiak daude eskuragarri."
      ],
      "metadata": {
        "id": "eURNZwARE40c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xHHEecc07S4PHxjzW9Uu166hldgQKnFj' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1xHHEecc07S4PHxjzW9Uu166hldgQKnFj\" -O datubasea.csv && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1frgaujo7d5LLX06tDZe8OHmGZHKzxfUQ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1frgaujo7d5LLX06tDZe8OHmGZHKzxfUQ\" -O datubaseaTEST.csv && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1djFRnHX7N3Mcc8hxXvNEv_UVXt5aptoY' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1djFRnHX7N3Mcc8hxXvNEv_UVXt5aptoY\" -O t5laburtzailea25epoch.pk && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1E03O0Bv-Y30UhSHi7n6kAX9pWdqIKut8' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1E03O0Bv-Y30UhSHi7n6kAX9pWdqIKut8\" -O RA_SFN.json && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YsZpnHh0MT4_m9Gd3b301mf6iitvIeOK' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1YsZpnHh0MT4_m9Gd3b301mf6iitvIeOK\" -O RA_ABS.json && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "V-u-sXrjFIVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datuak egokitzea\n",
        "\n",
        "Lehenengo lana T5 ereduarentzat datuak egokitzea da, honek .csv formatuan jasotzen baititu datuak eta gainera modu zehatz batean: lehenik laburpena eta gero artikulu orijinala. Bide batez, testua garbitzeko ere aprobetxatuko dugu exekuzioa.\n",
        "\n",
        "Azpiko kode-gelaxkak exekutatu nahi badira, lehenik datu-basearen fitxategiak eskuratu behar dira berriz, testu formatuan. Hartarako ondorengo gelaxka exekutatu."
      ],
      "metadata": {
        "id": "pWJo5h2ZW2Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1yQ653HXAqDWCFouevho8UnHRfVjLG2a3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1yQ653HXAqDWCFouevho8UnHRfVjLG2a3\" -O t5Aurre.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -q t5Aurre.zip "
      ],
      "metadata": {
        "id": "F6iVj1H9iiwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitxategi guztiak eskuratu, hau da, kode hau exekutatzen den tokian datu-basearen train, val eta test partizioak egon behar dira bakoitza fitxategi batean kokatuta.\n",
        "def eskuratu_fitxategi_guztiak (karpeta):\n",
        "    root_dir = karpeta #Karpeta\n",
        "    train = [ ]\n",
        "    test = [ ]\n",
        "    val = [ ]\n",
        "    for dir_, _, files in os.walk(root_dir): #Fitxategi guztiak zeintzuk diren ikusi\n",
        "        for file_name in files: #Karpetako fitxategi bakoitzeko\n",
        "            rel_dir = os.path.relpath(dir_, root_dir) #Bere bide-izen erlatiboa eskuratu\n",
        "            rel_file = os.path.join(rel_dir, file_name)\n",
        "            if (\"train\" in file_name): #Atzeman entrenamenduko partizioaren fitxategia.\n",
        "                train.append(karpeta + \"/\" + rel_file) #Testeko fitxategia sortzeko HAU KOMENTATU\n",
        "            elif (\"val\" in file_name): #Balidazioko partizioaren fitxategia.\n",
        "                val.append(karpeta + \"/\" +rel_file) #Testeko fitxategia sortzeko HAU KOMENTATU\n",
        "            else: #Testeko partizioaren fitxategia\n",
        "                test.append(karpeta + \"/\"+ rel_file) #Entrenamenduko eta balidazioko fitxategiak sortzeko HAU KOMENTATU\n",
        "    return([train,test,val])\n",
        "#Testua garbitzen duen funtzioa, ereduari sartuko zaio csv-a sortzeko.\n",
        "def clean_text(text):\n",
        "        text = text.lower() #Letra xehez ipini testua\n",
        "        text = re.sub('\\[.*?\\]', '', text) #Ordezkapenak egin.\n",
        "        text = re.sub('https?://\\S+|www\\.\\S+', '', text) #URLak kendu\n",
        "        text = re.sub('<.*?>+', '', text)\n",
        "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "        text = re.sub('\\n', '', text) #Lerro jauziak kendu\n",
        "        text = re.sub('\\w*\\d\\w*', '', text)\n",
        "        return text\n",
        "#Datuak eraldatzeko funtzioa, ereduari sartuko zaion .csv-a sortzeko. \n",
        "def eraldatuDatuak (karpeta):\n",
        "    dokumentuak=eskuratu_fitxategi_guztiak(karpeta) #Eskuratu fitxategi guztiak.\n",
        "    csva = [ ]\n",
        "    g = open(\"/content/datubaseaBerria.csv\",\"w\") #Emaitza jasoko duen fitxategia ireki.\n",
        "    g.write(\"headlines,text\\n\") #Jarri goiburua bertan.\n",
        "    for i in tqdm(dokumentuak): #Dokumentu bakoitzeko egin.\n",
        "        with ExitStack() as stack: #Fitxategi batean laburpenak daude, bestean jatorrizko testuak eta hori partizio bakoitzeko. \n",
        "            files = [stack.enter_context(open(fname)) for fname in i] #Laburpenak jatorrizko fitxategiekin elkartu, haien artean desordenatu gabe. \n",
        "            for lines in tqdm(zip_longest(*files)): #Lerroka idatzi csv-an, csv-ko lerro bakoitzak entrenamendu edo test adibide bat adieraziko du.\n",
        "                testua=clean_text(str(lines[0])) #Garbitu testua\n",
        "                laburpena=clean_text(str(lines[1]))\n",
        "                g.write(str(laburpena+\",\"+testua+\"\\n\")) #Idatzi testua\n",
        "    g.close()\n",
        "            \n",
        "    \n",
        "eraldatuDatuak(\"/content/t5Aurre\")"
      ],
      "metadata": {
        "id": "hOpSpDeaStFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goiko kodea erabilita sor daitezkeen datu-base eraldatuak jada sortuta daude eta lehenago jaitsi ditugu. Adi! Goiko kodea dagoen bezala exekutatuz gero, fitxategi bakarra sortuko da jatorrizko datu-baseko entrenamendu, balidazio eta testeko adibideak nahastuta. Hori ez dugu nahi, bi fitxategi nahi ditugu: ereduari entrenamenduan ematekoak (entrenamendu eta balidazioa) eta testekoak (testa). Hori egiteko kodea iruzkinetan esanda dagoen moduan komentatu edo deskomentatu behar da. "
      ],
      "metadata": {
        "id": "IbxKYcA1CxZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5 ereduaren definizioa\n",
        "\n",
        "Jarraian dagoen kodeak T5 eredua definitzen du, gure atazara egokituta."
      ],
      "metadata": {
        "id": "ZJgLJWbbXT6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings:\n",
        "#     PROJ_NAME = 'Text-Summarization-Using-T5'\n",
        "#     root_path = os.getcwd().split(PROJ_NAME)[0] + PROJ_NAME + \"\\\\\"\n",
        "#     APPLICATION_PATH = root_path + \"backend\\\\services\\\\text_summarization\\\\application\\\\\"\n",
        "    # setting up logs path\n",
        "#     LOGS_DIRECTORY = root_path + \"backend\\\\services\\\\text_summarization\\\\logs\\\\logs.txt\"\n",
        "\n",
        "    MODEL_TYPE = \"t5\"\n",
        "    MODEL_NAME = \"t5-small\" #Erabiliko dugun eredua t5-txikia izango da\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #GPUa erabilgarri dagoen edo ez begiratu eta hala bada erabili.\n",
        "\n",
        "    # Entrenamenduko datuen fitxategia.\n",
        "    TRAIN_DATA = \"/content/datubasea.csv\"\n",
        "\n",
        "    Columns = ['headlines', 'text']\n",
        "\n",
        "    USE_GPU = None\n",
        "    if str(DEVICE) == \"cuda\":\n",
        "        USE_GPU=True\n",
        "    else:\n",
        "        USE_GPU = False\n",
        "\n",
        "    EPOCHS = 25 #Zehaztu aro kopurua.\n",
        "    #Zehaztu hiperparametroak.\n",
        "    encoding = 'latin-1'\n",
        "    columns_dict = {\"headlines\": \"target_text\", \"text\": \"source_text\"}\n",
        "    df_column_list = ['source_text', 'target_text']\n",
        "    SUMMARIZE_KEY = \"summarize: \"\n",
        "    SOURCE_TEXT_KEY = 'source_text'\n",
        "    TEST_SIZE = 0.2\n",
        "    BATCH_SIZE = 8\n",
        "    source_max_token_len = 128\n",
        "    target_max_token_len = 50\n",
        "    train_df_len = 5000\n",
        "    test_df_len = 100"
      ],
      "metadata": {
        "id": "DNtJaZ23XbRM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocess:\n",
        "    def __init__(self):\n",
        "        self.settings = Settings\n",
        "    \n",
        "    #Aurrez azaldu da funtzio hau.\n",
        "    def clean_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub('\\[.*?\\]', '', text)\n",
        "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "        text = re.sub('<.*?>+', '', text)\n",
        "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "        text = re.sub('\\n', '', text)\n",
        "        text = re.sub('\\w*\\d\\w*', '', text)\n",
        "        return text\n",
        "    #Datuak aurreprozesatzeko funtzioa.\n",
        "    def preprocess_data(self, data_path):\n",
        "        df = pd.read_csv(data_path, encoding=self.settings.encoding, usecols=self.settings.Columns,sep=',')\n",
        "        # simpleT5ek bi zutabeko datu-egitura espero du: \"source_text\" eta \"target_text\"\n",
        "        df = df.rename(columns=self.settings.columns_dict)\n",
        "        df = df[self.settings.df_column_list]\n",
        "        # T5 ereduak atazarekin lotutako aurrizki bat espero du: laburpengintzan ari garenez \"summarize: \" aurrizkia erabiliko dugu.\n",
        "        df[self.settings.SOURCE_TEXT_KEY] = self.settings.SUMMARIZE_KEY + df[self.settings.SOURCE_TEXT_KEY]\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "vKeVO6ZJXkMF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eredua definitzen duen klasea.\n",
        "class T5Model:\n",
        "    def __init__(self, model_type, model_name):\n",
        "        self.model = SimpleT5()\n",
        "        self.model.from_pretrained(model_type=model_type,\n",
        "                                   model_name=model_name)\n",
        "\n",
        "    def load_model(self, model_type, model_path, use_gpu: bool):\n",
        "        try:\n",
        "            self.model.load_model(\n",
        "                model_type=model_type,\n",
        "                model_dir=model_path,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "\n",
        "        except BaseException as ex:\n",
        "            print(\"error occurred while loading model \", str(ex))"
      ],
      "metadata": {
        "id": "PGe-5LfrXdxk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamendu prozedura definitzea\n",
        "\n",
        "Entrenamendu prozedura definitzen da azpiko gelaxkan. T5 klasearen esku uzten dira entrenamenduan lantzen diren ohiko xehetasun asko."
      ],
      "metadata": {
        "id": "YNT6gzQZXlA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamendua definitzen duen prozedura\n",
        "class Train:\n",
        "    def __init__(self):\n",
        "        # Hasieratu klasea\n",
        "        self.settings = Settings\n",
        "        self.preprocess = Preprocess()\n",
        "\n",
        "        # Hasieratu aldagaiak \n",
        "        self.t5_model = None\n",
        "    #Hasieraketak, lehenago definitutako klasearen arabera.\n",
        "    def __initialize(self):\n",
        "        try:\n",
        "            self.t5_model = T5Model(model_name=self.settings.MODEL_NAME,\n",
        "                                    model_type=self.settings.MODEL_TYPE)\n",
        "\n",
        "        except BaseException as ex:\n",
        "            print(\"error occurred while loading model \", str(ex))\n",
        "\n",
        "    def set_seed(self, seed_value=42):\n",
        "        random.seed(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        torch.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "    #Entrenamendua burutzeko funtzioa\n",
        "    def train(self, df):\n",
        "        try:\n",
        "            train_df, test_df = train_test_split(df, test_size=self.settings.TEST_SIZE)\n",
        "\n",
        "            self.t5_model.model.train(train_df=train_df[:self.settings.train_df_len],\n",
        "                                      eval_df=test_df[:self.settings.test_df_len],\n",
        "                                      source_max_token_len=self.settings.source_max_token_len,\n",
        "                                      target_max_token_len=self.settings.target_max_token_len,\n",
        "                                      batch_size=self.settings.BATCH_SIZE, max_epochs=self.settings.EPOCHS,\n",
        "                                      use_gpu=self.settings.USE_GPU)\n",
        "\n",
        "        except BaseException as ex:\n",
        "            print(\"error occurred while loading model \", str(ex))\n",
        "    #Exekuzioa egiteko funtzioa.\n",
        "    def run(self):\n",
        "        try:\n",
        "            print(\"Loading and Preparing the Dataset-----!! \")\n",
        "            df = self.preprocess.preprocess_data(self.settings.TRAIN_DATA) #Aurreprozesaketa.\n",
        "            print(df.head())\n",
        "            print(\"Dataset Successfully Loaded and Prepared-----!! \")\n",
        "            print(\"Loading and Initializing the T5 Model -----!! \")\n",
        "            self.__initialize() #Hasieraketa.\n",
        "            print(\"Model Successfully Loaded and Initialized-----!! \")\n",
        "\n",
        "            print(\"------------------Starting Training-----------!!\")\n",
        "            self.set_seed()\n",
        "            self.train(df) #Entrenamendua\n",
        "            print(\"Training complete-----!!!\")\n",
        "            torch.save(self.t5_model.model.T5Model.state_dict,\"/content/t5laburtzailea25epochB.pk\") #Gorde eredua.\n",
        "\n",
        "        except BaseException as ex:\n",
        "            print(\"Following Exception Occurred---!! \", str(ex))"
      ],
      "metadata": {
        "id": "kbah9GSoXzEm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "t= Train()\n",
        "t.run()"
      ],
      "metadata": {
        "id": "8gUIzC-WX11u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eredua probatzea\n",
        "\n",
        "Kargatu dezagun jada entrenatuta dagoen eredu bat eta probatu test partizioko adibide batekin. "
      ],
      "metadata": {
        "id": "jsvous4M5uwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t5_model = T5Model(model_name=Settings.MODEL_NAME,model_type=Settings.MODEL_TYPE)\n",
        "t5_model.model.load_model(\"/content/t5laburtzailea25epoch.pk\")"
      ],
      "metadata": {
        "id": "8xoF3AAE5xyO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proba=t5_model.model.predict(dataset[\"test\"][0][\"document\"])\n",
        "print(proba)"
      ],
      "metadata": {
        "id": "y1xmp0LwB5e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE-2 analisia\n",
        "\n",
        "Ikusten den bezala, eredua iragarpenak egiteko gai da. Orain iragarpen horien kalitatea neurtzeko ordua da. Iragarpenak neurtzeko ROUGE-2 metrika erabiliko dugu, aurreko eredu erauzlearekin egin bezala. "
      ],
      "metadata": {
        "id": "D0_TRirxCDxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rougeAnalisia (eredua,test_datuak,zenbat,rouge):\n",
        "    jsona = {} #Hasieraketak\n",
        "    f1ak = [ ]\n",
        "    estaldurak = [ ]\n",
        "    zehaztasunak = [ ]\n",
        "    kont=0\n",
        "    for i in tqdm(test_datuak): #Dokumentu hautatu bakoitzeko egin:\n",
        "        if (kont == 0):\n",
        "          kont+=1\n",
        "          continue\n",
        "        elif (kont == zenbat+1):\n",
        "          break\n",
        "        dokumentua=i[1] #Artikulu orijinala eskuratu.\n",
        "        laburpena=i[0] #Jatorrizko laburpena eskuratu.\n",
        "        iragarria=eredua.predict(dokumentua)[0]\n",
        "        rougeak = rouge.get_scores(iragarria,laburpena)[0][\"rouge-2\"] #Rouge-2a kalkulatu\n",
        "        f1ak.append(rougeak[\"f\"]) #F1a gorde\n",
        "        estaldurak.append(rougeak[\"r\"]) #Recall-a gorde\n",
        "        zehaztasunak.append(rougeak[\"p\"]) #Precision-a gorde\n",
        "        jsona[str(kont)]=[kont,iragarria,laburpena,rougeak[\"f\"],rougeak[\"r\"],rougeak[\"p\"]] #Denak bildu zerrenda batean\n",
        "        kont+=1\n",
        "    f1bb = sum(f1ak)/len(f1ak) #Batez bestekoak atera F1etan.\n",
        "    estaldurakbb = sum(estaldurak) / len(estaldurak) #Berdina recall-etan.\n",
        "    zehaztasunakbb = sum(zehaztasunak) / len(zehaztasunak) #Berdina precision-etan.\n",
        "    with open('rougeAnalisiaAbs.json', 'w') as f: #Gorde dokumentu guztien datuak fitxategi batean.\n",
        "        json.dump(jsona, f)\n",
        "    return ([f1bb,estaldurakbb,zehaztasunakbb],f1ak, estaldurak, zehaztasunak) #Itzuli ROUGE-2aren datuak soilik."
      ],
      "metadata": {
        "id": "lX0Vi-h3CSvs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "f = open('/content/datubaseaTEST.csv', newline='')\n",
        "spamreader = csv.reader(f, delimiter=',', quotechar='|') #Irakurri csva\n",
        "bbak, f1ak, estaldurak, zehaztasunak = rougeAnalisia(t5_model.model,spamreader,30,rouge)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "pVJfBJ2STj7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ereduen arteko alderaketa\n",
        "\n",
        "Azkenik, sortu ditugun bi ereduak alderatzea falta zaigu. ROUGE-2 analisia egitean eredu bakoitzari 30 laburpen sortzea eskatu diogu. Lagin egokia izan daiteke ereduen gaitasunak aztertzeko eta alderatzeko. \n",
        "\n",
        "Jarraian 30 laburpen horiek berreskuratuko ditugu.  "
      ],
      "metadata": {
        "id": "vDEf_qX_M-Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/RA_SFN.json\",\"r\") as f:\n",
        "  erauzketa=json.loads(f.read())\n",
        "with open(\"/content/RA_ABS.json\") as g:\n",
        "  abstrakzioa=json.loads(g.read())"
      ],
      "metadata": {
        "id": "HWD520WfNYU5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(erauzketa),erauzketa)\n",
        "print(len(abstrakzioa),abstrakzioa)"
      ],
      "metadata": {
        "id": "Zn3m4uGwR-8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Berrordenaketa funtzioa, datuak txukun inprimatzeko.\n",
        "def berrordenatu (erauzketa,abstrakzioa,testa):\n",
        "  f1ak= [ ]\n",
        "  rec = [ ]\n",
        "  prec = [ ]\n",
        "  laburpenak = [ ]\n",
        "  kont=0\n",
        "  for i in erauzketa: #Datu egitura honetan f1, rec, prec eta laburpenentzako datu egitura bana sortu, tuplez osatua, bakoitzak erauzketaren emaitza eta abstrakzioaren emaitza izango ditu, hurrenez hurren.\n",
        "    f1ak.append((erauzketa[i][3],abstrakzioa[str(int(i)+1)][3]))\n",
        "    rec.append((erauzketa[i][4],abstrakzioa[str(int(i)+1)][4]))\n",
        "    prec.append((erauzketa[i][5],abstrakzioa[str(int(i)+1)][5]))\n",
        "    laburpenak.append((testa[kont][\"document\"],erauzketa[i][2],erauzketa[i][1],abstrakzioa[str(int(i)+1)][1]))\n",
        "    kont+=1\n",
        "  return (f1ak, rec, prec, laburpenak)\n",
        "#Inprimaketa txukuna egiteko\n",
        "def inprimaketaTxukuna (bektorea,titulua1,titulua2):\n",
        "  print(\"Zbk.\"+ \"       \" +titulua1 + \"       \" + titulua2) #Izenburuak jartzeko.\n",
        "  kont=1\n",
        "  for i in bektorea: #Errenkada bakoitzean haren zenbakia, erauzketari dagokion datua eta abstrakzioari dagokiona.\n",
        "    print(str(kont) + \"       \" + str(i[0]) + \"       \" + str(i[1]))\n",
        "    kont+=1\n",
        "\n",
        "def banandu (zerrenda):\n",
        "  bat = [ ]\n",
        "  bi = [ ]\n",
        "  for i in zerrenda:\n",
        "    bat.append(i[0])\n",
        "    bi.append(i[1])\n",
        "  return bat, bi"
      ],
      "metadata": {
        "id": "QX0McqkPgnXT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ikus ditzagun ereduen ROUGE-2 indizeak zeintzuk diren 30 laburpen horietako bakoitzean."
      ],
      "metadata": {
        "id": "Vr-boeXth_5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1ak, rec, prec, laburpenak = berrordenatu(erauzketa,abstrakzioa,dataset[\"test\"])\n",
        "print(\"                         F1ak\\n\\n\")\n",
        "inprimaketaTxukuna(f1ak,\"Erauzketa\",\"Abstrakzioa\")\n",
        "print(\"\\n\\n                     Estaldurak\\n\\n\")\n",
        "inprimaketaTxukuna(rec,\"Erauzketa\",\"Abstrakzioa\")\n",
        "print(\"\\n\\n                     Doitasunak\\n\\n\")\n",
        "inprimaketaTxukuna(prec,\"Erauzketa\",\"Abstrakzioa\")"
      ],
      "metadata": {
        "id": "XbGF0SuqjsOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Azkenik, inprimatu dezagun jatorrizko testu baten urrezko estandarreko laburpena, erauzketa bidezko sistemak sortutakoa eta abstrakzio bidezko sistemak sortutakoa."
      ],
      "metadata": {
        "id": "TNVFNhh9iHex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in laburpenak:\n",
        "  print(str(i[0])) #Jatorrizko dokumentua\n",
        "  print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
        "  print(i[1]) #Urrezko estandarreko laburpena\n",
        "  print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
        "  print(i[2]) #Erauzketa bidezko sistemaren laburpena\n",
        "  print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
        "  print(i[3]) #Abstrakzio bidezko sistemaren laburpena\n",
        "  print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "C0d27jR6msg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}